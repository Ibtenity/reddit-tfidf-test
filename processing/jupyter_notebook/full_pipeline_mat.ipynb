{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, collect_list, udf, length, col, regexp_replace, lower\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, IDF\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Normalizer\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRow, IndexedRowMatrix, CoordinateMatrix\n",
    "\n",
    "from numpy import delete\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .enableHiveSupport()\\\n",
    "    .appName(\"tfidf-reddit-pipeline\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path100K = \"RC100K_2019-07\"\n",
    "path300K = \"RC300K_2019-07\"\n",
    "path1M = \"RC1M_2019-07\"\n",
    "path5M = \"RC5M_2019-07\"\n",
    "path10M = \"RC10M_2019-07\"\n",
    "path15M = \"RC15M_2019-07\"\n",
    "\n",
    "nwords=100\n",
    "nsubreddits=100\n",
    "mindf=1.0\n",
    "minlength=50000\n",
    "vocabsize=20000\n",
    "\n",
    "\n",
    "path = path10M\n",
    "df = spark.read.json(path + '.txt')\n",
    "#commentsDF = spark.read.json(path100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Stefan_Fairphone \n",
    "# https://stackoverflow.com/questions/45881580/pyspark-rdd-sparse-matrix-multiplication-from-scala-to-python\n",
    "def coordinateMatrixMultiply(leftmatrix, rightmatrix):\n",
    "    left  =  leftmatrix.entries.map(lambda e: (e.j, (e.i, e.value)))\n",
    "    right = rightmatrix.entries.map(lambda e: (e.i, (e.j, e.value)))\n",
    "    productEntries = left \\\n",
    "        .join(right) \\\n",
    "        .map(lambda e: ((e[1][0][0], e[1][1][0]), (e[1][0][1]*e[1][1][1]))) \\\n",
    "        .reduceByKey(lambda x,y: x+y) \\\n",
    "        .map(lambda e: (*e[0], e[1]))\n",
    "    return productEntries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor(Transformer):\n",
    "    \"\"\"\n",
    "    Concatenate all of the comment strings belonging to each subreddit into a big string\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key=None, val=None, inputCol=None, outputCol=None):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "\n",
    "    def transform(self, df):\n",
    "        return df.groupby(self.key).agg(concat_ws(\" \", collect_list(self.val)).alias(self.outputCol))\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        return self.outputCol\n",
    "\n",
    "    def getinputCol(self):\n",
    "        return self.inputCol\n",
    "\n",
    "\n",
    "class Filterer(Transformer):\n",
    "    \"\"\"\n",
    "    Filter out the subreddits whose 'document' string is less than args.minlength\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key=None, val=None, inputCol=None, outputCol=None, minlength=None):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "        self.minlength = minlength\n",
    "\n",
    "    def transform(self, df):\n",
    "        return df.filter((length(self.outputCol)) >= self.minlength)\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        return self.outputCol\n",
    "\n",
    "    def getinputCol(self):\n",
    "        return self.inputCol\n",
    "\n",
    "\n",
    "class Cleaner(Transformer):\n",
    "    \"\"\"\n",
    "    Remove all non whitespace or non alphabetical characters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key=None, val=None, inputCol=None, outputCol=None):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "\n",
    "    def transform(self, df):\n",
    "        return df.select(self.key, (lower(regexp_replace(self.val, \"[^a-zA-Z\\\\s]\", \"\")).alias(self.outputCol)))\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        return self.outputCol\n",
    "\n",
    "    def getinputCol(self):\n",
    "        return self.inputCol\n",
    "\n",
    "\n",
    "class TopKWords(Transformer):\n",
    "    \"\"\"\n",
    "    find the k words with greatest tf-idf for each subreddit\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key=None, val=None, inputCol=None, outputCol=None, nwords=5):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "        self.nwords = nwords\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        return self.outputCol\n",
    "\n",
    "    def getinputCol(self):\n",
    "        return self.inputCol\n",
    "\n",
    "    def transform(self, df):\n",
    "        words_schema = StructType([\n",
    "            StructField('tfidfs', ArrayType(FloatType()), nullable=False),\n",
    "            StructField('words', ArrayType(StringType()), nullable=False)\n",
    "        ])\n",
    "\n",
    "        def getTopKWords(x, k=5):\n",
    "            tfidfs = x.toArray()\n",
    "            indices = tfidfs.argsort()[-k:][::-1]\n",
    "            return tfidfs[indices].tolist(), [vocab[i] for i in indices]\n",
    "\n",
    "        topkwords_udf = udf(lambda x: getTopKWords(x, k=self.nwords), words_schema)\n",
    "\n",
    "        return df.withColumn('top_words', topkwords_udf(col('tfidf')))\n",
    "\n",
    "\n",
    "class CosineSimilarity(Transformer):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between tfidf vectors of all subreddit pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key=None, val=None, inputCol=None, outputCol=None, spark=None):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "        self.spark = spark\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        return self.outputCol\n",
    "\n",
    "    def getinputCol(self):\n",
    "        return self.inputCol\n",
    "\n",
    "    def transform(self, df):\n",
    "        # add a row index, starting from 0 (to be used for matrix computations, i.e. cosine similarity)\n",
    "        df.createOrReplaceTempView('data')\n",
    "        data = self.spark.sql('select row_number() over (order by \"subreddit\") as index, * from data')\n",
    "        df = data.withColumn('index', col('index') - 1)\n",
    "        # normalize each tfidf vector to be unit length\n",
    "        normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"norm\")\n",
    "        df = normalizer.transform(df)\n",
    "        # compute matrix of tfidf cosine similarities, all distributed :D (why we use BlockMatrix)\n",
    "        # mat = IndexedRowMatrix(df.select('index', 'norm') \\\n",
    "        #                        .rdd.map(lambda x: IndexedRow(x['index'], x['norm'].toArray()))).toBlockMatrix()\n",
    "        # dot = mat.multiply(mat.transpose())\n",
    "        # cossimDF = dot.toIndexedRowMatrix().rows.toDF().withColumnRenamed('vector', 'cos_sims')\n",
    "        mat = IndexedRowMatrix(df.select('index', 'norm') \\\n",
    "                               .rdd.map(lambda x: IndexedRow(x['index'], x['norm'].toArray()))).toCoordinateMatrix()\n",
    "        cossim = CoordinateMatrix(coordinateMatrixMultiply(mat, mat.transpose())).toIndexedRowMatrix()\n",
    "        cossimDF = cossim.rows.toDF().withColumnRenamed('vector', 'cos_sims')\n",
    "\n",
    "        return df.join(cossimDF, ['index'])\n",
    "\n",
    "\n",
    "class TopKSubreddits(Transformer):\n",
    "    \"\"\"\n",
    "    For each subreddit, find the k other subreddits with greatest cosine similarity (of tf-idf vectors)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key=None, val=None, inputCol=None, outputCol=None, nsubreddits=5):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "        self.nsubreddits = nsubreddits\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        return self.outputCol\n",
    "\n",
    "    def getinputCol(self):\n",
    "        return self.inputCol\n",
    "\n",
    "    def transform(self, df):\n",
    "        subreddits_schema = StructType([\n",
    "            StructField('cos_sims', ArrayType(FloatType()), nullable=False),\n",
    "            StructField('subreddits', ArrayType(StringType()), nullable=False)\n",
    "        ])\n",
    "\n",
    "        # index_map is going to be in the driver local memory, it's generally not too big\n",
    "        index_map = df.select('index', 'subreddit').toPandas().set_index('index')['subreddit'].to_dict()\n",
    "\n",
    "        def getTopKSubreddits(x, k=5):\n",
    "            # so we can skip the obvious most similar subreddit (itself)\n",
    "            k += 1\n",
    "            cos_sims = x.toArray()\n",
    "            indices = cos_sims.argsort()[-k:][::-1]\n",
    "            indices = delete(indices, 0)  # delete that first element which is the subreddit itself\n",
    "            return cos_sims[indices].tolist(), [index_map[i] for i in indices]\n",
    "\n",
    "        topksubreddits_udf = udf(lambda x: getTopKSubreddits(x, k=self.nsubreddits), subreddits_schema)\n",
    "\n",
    "        return df.withColumn('top_subreddits', topksubreddits_udf(col('cos_sims')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the ETL pipeline\n",
    "extractor = Extractor(key='subreddit', val='body', inputCol='subreddit', outputCol='body')\n",
    "cleaner = Cleaner(key='subreddit', val='body', inputCol=extractor.getOutputCol(), outputCol='body')\n",
    "filterer = Filterer(key='subreddit', val='body', inputCol='subreddit', outputCol='body', minlength=minlength)\n",
    "tokenizer = RegexTokenizer(inputCol=cleaner.getOutputCol(), outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"swr_tokens\")\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"tf\", minDF=mindf, vocabSize=vocabsize)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"tfidf\")\n",
    "topkwords = TopKWords(inputCol=idf.getOutputCol(), outputCol='top_words', nwords=nwords)\n",
    "cos_similarity = CosineSimilarity(inputCol='subreddit', outputCol='norm', spark=spark)\n",
    "topksubreddits = TopKSubreddits(inputCol=cos_similarity.getOutputCol(), outputCol='top_subreddits',\n",
    "                                nsubreddits=nsubreddits)\n",
    "\n",
    "pipeline = Pipeline(stages=[extractor, cleaner, filterer, tokenizer, remover, cv, idf, topkwords \\\n",
    "    , cos_similarity, topksubreddits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model, thene extract the learned vocabulary\n",
    "model = pipeline.fit(df)\n",
    "stages = model.stages\n",
    "vectorizers = [s for s in stages if isinstance(s, CountVectorizerModel)]\n",
    "vocab = vectorizers[0].vocabulary\n",
    "# compute the tfidfs\n",
    "df = model.transform(df)\n",
    "df = df.drop('body', 'tf', 'tokens', 'swr_tokens', 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_table = str.maketrans('', '', '-_')\n",
    "df.write.mode('overwrite').saveAsTable(path.translate(trans_table))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
